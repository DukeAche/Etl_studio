# ğŸš€ No-Code ETL Studio - Deployment Guide

## Quick Start (5 minutes)

### Option 1: Automated Setup
```bash
# Make script executable and run
chmod +x setup_and_run.sh
./setup_and_run.sh
```

### Option 2: Manual Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Run the application
streamlit run app.py
```

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip package manager
- 2GB RAM minimum (4GB recommended for large datasets)
- Modern web browser (Chrome, Firefox, Safari, Edge)

## ğŸ¯ Core Features Implemented

### âœ… Phase 1: Source (Ingestion)
- **File Upload**: CSV, Excel, JSON, Parquet support with drag-and-drop
- **Database Connection**: SQLAlchemy-compatible connections
- **Data Preview**: Real-time preview with metadata
- **Caching**: Optimized data loading with `@st.cache_data`

### âœ… Phase 2: Profile (Quality Check)
- **Data Health Score**: Comprehensive quality metric (0-100%)
- **Missing Values Analysis**: Interactive heatmap and bar charts
- **Duplicate Detection**: Row-level duplicate identification
- **Schema Analysis**: Complete column information and data types

### âœ… Phase 3: SQL Workbench (Killer Feature)
- **DuckDB Integration**: Full SQL support on DataFrames
- **Advanced Editor**: Syntax highlighting with streamlit-ace
- **Multi-Table Queries**: Query across all loaded DataFrames
- **Query History**: Save and reuse previous queries
- **SQL Templates**: Pre-built examples for common operations

### âœ… Phase 4: Transform
- **No-Code Operations**:
  - Drop duplicates with one click
  - Fill missing values (forward fill, backward fill, zero, mean, median)
  - Trim whitespace from string columns
- **Column Management**: Rename columns with UI
- **Type Casting**: Convert between data types
- **Real-time Preview**: See changes before applying

### âœ… Phase 5: Sink (Export)
- **Multiple Formats**: CSV, Parquet, JSON, Excel
- **Compression**: gzip, zip, bz2, xz support
- **Quality Metrics**: Final data assessment
- **Transaction Log**: Complete audit trail export

## ğŸ—ï¸ Architecture Highlights

### Session State Management
```python
# Core state variables maintained across operations
- transaction_log: Complete audit trail
- current_df: Active DataFrame reference
- dataframes: Dictionary of all datasets
- query_history: SQL query preservation
```

### Performance Optimizations
- **Caching**: Data loading cached with Streamlit's cache_data
- **Lazy Evaluation**: Efficient memory management
- **Progress Indicators**: Real-time operation feedback
- **Memory Monitoring**: Built-in usage tracking

## ğŸ“Š Sample Data Included

The package includes `sample_customer_data.csv` with:
- 1,020 records (1,000 original + 20 duplicates)
- 10 columns including numeric, string, date, and boolean types
- Realistic data quality issues for testing:
  - 50 missing values in age column (5%)
  - 20 duplicate rows
  - Whitespace issues in customer names

## ğŸ§ª Testing Your Installation

### Test 1: Basic Functionality
1. Upload the sample CSV file
2. Check data profiling shows quality issues
3. Run a simple SQL query: `SELECT * FROM df LIMIT 5`
4. Apply a transformation (drop duplicates)
5. Export the cleaned data

### Test 2: SQL Workbench
1. Load sample data
2. Try the demo SQL queries from `demo_sql_queries.sql`
3. Save query results as new DataFrame
4. Switch between DataFrames using sidebar selector

### Test 3: End-to-End Pipeline
1. Upload raw data with issues
2. Profile to identify problems
3. Use SQL to filter and analyze
4. Transform to clean data
5. Export gold standard dataset

## ğŸ”§ Configuration Options

### Streamlit Configuration
The app runs with these optimized settings:
```bash
streamlit run app.py \
  --server.port=8501 \
  --server.address=localhost \
  --server.maxUploadSize=2000  # 2GB file upload limit
```

### Memory Management
- Large datasets automatically trigger performance warnings
- Memory usage displayed in real-time
- Efficient data structures minimize RAM usage

## ğŸ› Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Reinstall dependencies
   pip uninstall streamlit pandas duckdb
   pip install -r requirements.txt
   ```

2. **Memory Issues**
   - Reduce dataset size or use sampling
   - Close other applications
   - Increase system virtual memory

3. **SQL Syntax Errors**
   - Check table names in schema explorer
   - Use the SQL examples provided
   - Verify DuckDB syntax compatibility

4. **Port Already in Use**
   ```bash
   # Change port
   streamlit run app.py --server.port=8502
   ```

### Performance Tips
- Use Parquet format for large datasets
- Apply filters early in pipeline
- Cache expensive operations
- Monitor memory usage indicators

## ğŸ“ˆ Scaling & Production

### For Large Datasets (>1GB)
- Use Parquet format instead of CSV
- Process data in chunks
- Consider sampling for initial exploration
- Monitor system resources

### For Enterprise Use
- Deploy on cloud platforms (AWS, Azure, GCP)
- Use containerization (Docker)
- Implement authentication and authorization
- Add data encryption for sensitive datasets

## ğŸ¯ Next Steps

1. **Immediate**: Test with the sample data provided
2. **Development**: Try your own datasets
3. **Advanced**: Explore complex SQL queries and transformations
4. **Integration**: Connect to databases for live data processing

## ğŸ“š Additional Resources

- **Demo Queries**: `demo_sql_queries.sql` contains 10 example queries
- **Sample Data**: `sample_customer_data.csv` for testing
- **Documentation**: Complete README.md with detailed usage guide
- **Source Code**: Well-commented `app.py` with clear architecture

## ğŸ¤ Support

The application includes:
- Built-in help tooltips
- Error handling with helpful messages
- Transaction logging for debugging
- Quality metrics for troubleshooting

---

**ğŸ‰ Your No-Code ETL Studio is ready to use!**

Start by running `./setup_and_run.sh` and navigate to `http://localhost:8501` in your browser.